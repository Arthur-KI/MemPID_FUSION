# chat_fusion_v3.py
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  ğŸ§  MemPID_FUSION v2 - Interactive Chat & Test
#
#  Author: Arthur-KI
#  License: MIT
#  GitHub: https://github.com/Arthur-KI/MemPID_FUSION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import torch
import torch.nn.functional as F
from tokenizers import Tokenizer

# Import model class from training script
from training_MemPID_FUSION_v2 import MemPIDModel

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SETUP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
MODEL_PATH = 'best_model_v2.pt'
TOKENIZER_PATH = 'tokenizer_v2.json'

print("â•" * 60)
print("  ğŸ§  MemPID_FUSION v2 - Chat & Test")
print("â•" * 60)
print(f"\nğŸ“‚ Lade Modell von {MODEL_PATH}...")

# Tokenizer laden
tokenizer = Tokenizer.from_file(TOKENIZER_PATH)

# Checkpoint laden
checkpoint = torch.load(MODEL_PATH, map_location=DEVICE, weights_only=False)
config = checkpoint['config']

print(f"   Config: DIM={config['dim']}, LAYERS={config['layers']}")
print(f"   Vocab: {config['vocab_size']}")
print(f"   Val Loss: {checkpoint['val_loss']:.4f}")

# Modell erstellen
model = MemPIDModel(
    vocab_size=config['vocab_size'],
    dim=config['dim'],
    layers=config['layers'],
    kernel_size=config['kernel'],
    batch_size=1
).to(DEVICE)

# State Dict bereinigen (Trainings-Buffer entfernen)
state_dict = checkpoint['model']
clean_state_dict = {}

for key, value in state_dict.items():
    # Ãœberspringe Trainings-Buffer
    if "integ" in key or "prev" in key:
        continue
    if "memory_p" in key or "memory_i" in key or "step" in key:
        continue
    clean_state_dict[key] = value

model.load_state_dict(clean_state_dict, strict=False)
model.eval()

print(f"\nâœ… Modell geladen!")
print("â•" * 60)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GENERATION FUNKTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@torch.no_grad()
def generate(
    prompt: str = "",
    category: str = None,
    max_tokens: int = 200,
    temperature: float = 0.8,
    top_k: int = 50,
    top_p: float = 0.9,
    show_progress: bool = True
):
    """
    Generiert Text basierend auf Prompt und/oder Kategorie.
    
    Args:
        prompt: Starttext (optional)
        category: Eine von <KLASSIKER>, <GESETZE>, <LYRIK>, <PHILOSOPHIE>, <WISSEN>
        max_tokens: Maximale Anzahl neuer Tokens
        temperature: KreativitÃ¤t (0.1=konservativ, 1.0=kreativ)
        top_k: Nur die k wahrscheinlichsten Tokens
        top_p: Nucleus Sampling Schwelle
        show_progress: Zeige Fortschritt
    
    Returns:
        Generierter Text
    """
    model.eval()
    model.reset_states()
    
    # Starte mit Kategorie-Token und/oder Prompt
    tokens = []
    
    if category:
        cat_id = tokenizer.token_to_id(category)
        if cat_id is not None:
            tokens.append(cat_id)
        else:
            print(f"âš ï¸ Kategorie {category} nicht gefunden!")
    
    if prompt:
        encoded = tokenizer.encode(prompt).ids
        tokens.extend(encoded)
    
    if not tokens:
        # Fallback: Starte mit <KLASSIKER>
        tokens = [tokenizer.token_to_id("<KLASSIKER>")]
    
    idx = torch.tensor([tokens], device=DEVICE)
    
    # Generation Loop
    iterator = range(max_tokens)
    if show_progress:
        from tqdm import tqdm
        iterator = tqdm(iterator, desc="Generiere", leave=False)
    
    for _ in iterator:
        model.reset_states()
        
        # Kontext begrenzen
        idx_cond = idx if idx.size(1) <= 2048 else idx[:, -2048:]
        
        # Forward
        logits, _ = model(idx_cond)
        logits = logits[:, -1, :] / temperature
        
        # Top-K Filtering
        if top_k > 0:
            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
            logits[logits < v[:, [-1]]] = float('-inf')
        
        # Top-P (Nucleus) Filtering
        if top_p < 1.0:
            sorted_logits, sorted_indices = torch.sort(logits, descending=True)
            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
            sorted_indices_to_remove = cumulative_probs > top_p
            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
            sorted_indices_to_remove[..., 0] = 0
            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
            logits[indices_to_remove] = float('-inf')
        
        # Sample
        probs = F.softmax(logits, dim=-1)
        idx_next = torch.multinomial(probs, num_samples=1)
        idx = torch.cat((idx, idx_next), dim=1)
        
        # Stop bei EOS
        eos_id = tokenizer.token_to_id("<EOS>")
        if eos_id and idx_next.item() == eos_id:
            break
    
    # Dekodieren
    text = tokenizer.decode(idx[0].tolist())
    return text


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# INTERAKTIVER CHAT MODUS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def interactive_chat():
    """Interaktiver Chat-Modus"""
    
    print("\n" + "â•" * 60)
    print("  ğŸ’¬ INTERAKTIVER MODUS")
    print("â•" * 60)
    print("""
Befehle:
  /klassiker  - Generiere im Klassiker-Stil
  /gesetze    - Generiere im Gesetzes-Stil
  /lyrik      - Generiere im Lyrik-Stil
  /philosophie- Generiere im Philosophie-Stil
  /wissen     - Generiere im Wissens-Stil
  
  /temp X     - Setze Temperature (z.B. /temp 0.7)
  /tokens X   - Setze Max Tokens (z.B. /tokens 300)
  /lang       - Generiere langen Text (500 Tokens)
  
  /quit       - Beenden
  
Oder einfach Text eingeben als Prompt!
""")
    
    # Defaults
    temperature = 0.8
    max_tokens = 2000
    current_category = "<KLASSIKER>"
    
    while True:
        try:
            user_input = input("\nğŸ¤ Du: ").strip()
        except (KeyboardInterrupt, EOFError):
            print("\n\nğŸ‘‹ Auf Wiedersehen!")
            break
        
        if not user_input:
            continue
        
        # Befehle verarbeiten
        if user_input.startswith("/"):
            cmd = user_input.lower()
            
            if cmd == "/quit" or cmd == "/exit":
                print("\nğŸ‘‹ Auf Wiedersehen!")
                break
            
            elif cmd == "/klassiker":
                current_category = "<KLASSIKER>"
                print(f"ğŸ“š Kategorie: {current_category}")
                text = generate(category=current_category, max_tokens=max_tokens, temperature=temperature)
                print(f"\nğŸ¤– Modell:\n{'-'*40}\n{text}\n{'-'*40}")
            
            elif cmd == "/gesetze":
                current_category = "<GESETZE>"
                print(f"âš–ï¸ Kategorie: {current_category}")
                text = generate(category=current_category, max_tokens=max_tokens, temperature=temperature)
                print(f"\nğŸ¤– Modell:\n{'-'*40}\n{text}\n{'-'*40}")
            
            elif cmd == "/lyrik":
                current_category = "<LYRIK>"
                print(f"ğŸ­ Kategorie: {current_category}")
                text = generate(category=current_category, max_tokens=max_tokens, temperature=temperature)
                print(f"\nğŸ¤– Modell:\n{'-'*40}\n{text}\n{'-'*40}")
            
            elif cmd == "/philosophie":
                current_category = "<PHILOSOPHIE>"
                print(f"ğŸ¤” Kategorie: {current_category}")
                text = generate(category=current_category, max_tokens=max_tokens, temperature=temperature)
                print(f"\nğŸ¤– Modell:\n{'-'*40}\n{text}\n{'-'*40}")
            
            elif cmd == "/wissen":
                current_category = "<WISSEN>"
                print(f"ğŸ“– Kategorie: {current_category}")
                text = generate(category=current_category, max_tokens=max_tokens, temperature=temperature)
                print(f"\nğŸ¤– Modell:\n{'-'*40}\n{text}\n{'-'*40}")
            
            elif cmd.startswith("/temp "):
                try:
                    temperature = float(cmd.split()[1])
                    temperature = max(0.1, min(2.0, temperature))
                    print(f"ğŸŒ¡ï¸ Temperature: {temperature}")
                except:
                    print("âŒ UngÃ¼ltige Temperature! Beispiel: /temp 0.7")
            
            elif cmd.startswith("/tokens "):
                try:
                    max_tokens = int(cmd.split()[1])
                    max_tokens = max(10, min(1000, max_tokens))
                    print(f"ğŸ“ Max Tokens: {max_tokens}")
                except:
                    print("âŒ UngÃ¼ltige Anzahl! Beispiel: /tokens 300")
            
            elif cmd == "/lang":
                print(f"ğŸ“œ Generiere langen Text (500 Tokens)...")
                text = generate(category=current_category, max_tokens=500, temperature=temperature)
                print(f"\nğŸ¤– Modell:\n{'-'*40}\n{text}\n{'-'*40}")
            
            elif cmd == "/help":
                print(__doc__)
            
            else:
                print(f"âŒ Unbekannter Befehl: {cmd}")
        
        else:
            # User Input als Prompt verwenden
            print(f"\nâ³ Generiere Fortsetzung...")
            text = generate(
                prompt=user_input,
                category=current_category,
                max_tokens=max_tokens,
                temperature=temperature
            )
            print(f"\nğŸ¤– Modell:\n{'-'*40}\n{text}\n{'-'*40}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# QUICK TEST MODUS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def quick_test():
    """Schneller Test aller Kategorien"""
    
    print("\n" + "â•" * 60)
    print("  ğŸ§ª QUICK TEST - Alle Kategorien")
    print("â•" * 60)
    
    categories = [
        ("<KLASSIKER>", "ğŸ“š"),
        ("<GESETZE>", "âš–ï¸"),
        ("<LYRIK>", "ğŸ­"),
        ("<PHILOSOPHIE>", "ğŸ¤”"),
        ("<WISSEN>", "ğŸ“–"),
    ]
    
    for cat, emoji in categories:
        print(f"\n{emoji} {cat}:")
        print("-" * 50)
        
        text = generate(category=cat, max_tokens=150, temperature=0.8, show_progress=False)
        print(text[:500] + "..." if len(text) > 500 else text)
        print("-" * 50)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LANGER KONTEXT TEST
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def long_context_test():
    """Test ob Modell Kontext Ã¼ber lange Texte hÃ¤lt"""
    
    print("\n" + "â•" * 60)
    print("  ğŸ“œ LANGER KONTEXT TEST (500 Tokens)")
    print("â•" * 60)
    
    print("\nGeneriere langen Klassiker-Text...")
    text = generate(category="<KLASSIKER>", max_tokens=500, temperature=0.7)
    
    print("\n" + "â”€" * 60)
    print(text)
    print("â”€" * 60)
    
    # Einfache Analyse
    words = text.split()
    sentences = text.count('.') + text.count('!') + text.count('?')
    
    print(f"\nğŸ“Š Statistik:")
    print(f"   WÃ¶rter: {len(words)}")
    print(f"   SÃ¤tze: ~{sentences}")
    print(f"   Ã˜ WÃ¶rter/Satz: {len(words)/max(sentences,1):.1f}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    print("\n" + "â•" * 60)
    print("  WÃ„HLE MODUS:")
    print("â•" * 60)
    print("""
  1. ğŸ’¬ Interaktiver Chat
  2. ğŸ§ª Quick Test (alle Kategorien)
  3. ğŸ“œ Langer Kontext Test (500 Tokens)
  
  (oder 'q' zum Beenden)
""")
    
    while True:
        choice = input("Deine Wahl [1/2/3/q]: ").strip()
        
        if choice == '1':
            interactive_chat()
            break
        elif choice == '2':
            quick_test()
            break
        elif choice == '3':
            long_context_test()
            break
        elif choice.lower() == 'q':
            print("ğŸ‘‹ Auf Wiedersehen!")
            break
        else:
            print("âŒ Bitte 1, 2, 3 oder q eingeben!")
